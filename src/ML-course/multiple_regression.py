# -*- coding: utf-8 -*-
"""Copy of 03-02-Multiple Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bf_2aqfewaH0FrXcwontAM4KvSdTeAqj

# Multiple Regression

Simple Linear Regression:

$$y = \beta_0 + \beta_1X$$

Multiple Linear Regression:

$$y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ...$$

Well studied field in statistics

Focus will be on what is relevant for Data Science - practical and relevant for prediction
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston
# Statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import r2_score

boston_data = load_boston()

df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)

print (df.describe())
X = df
y = boston_data.target

"""
We need to add a constant term to allow statsmodel.api to calculate the bias / intercepts.
Otherwise the model will try to estimate the bias within the coefficients, making them less precise

$y = mX$
$y = mX + c$
"""

X_constant = sm.add_constant(X)

pd.DataFrame(X_constant)

# Jupyter displays docstring: sm.OLS?

model = sm.OLS(y, X_constant)

lr = model.fit()

print(lr.summary())

"""There are a lot of statistical tests and information. Mostly for the purpose of statistical analysis.

You do not need all of these for data science.

Data science focus is on prediction and having models that work on predicting real data. It is not concerned as much with  correct specifications of statistical problems.

## Model Statistical Outputs:

**Dep. Variable**: The dependent variable or target variable

**Model**: Highlight the model used to obtain this output. It is OLS here. Ordinary least squares / Linear regression

**Method**: The method used to fit the data to the model. Least squares

**No. Observations**: The number of observations

**DF Residuals**: The degrees of freedom of the residuals. Calculated by taking the number of observations less the number of parameters

**DF Model**: The number of estimated parameters in the model. In this case 13. The constant term is not included.

**R-squared**: This is the coefficient of determination. Measure of goodness of fit.
$$R^2=1-\frac{SS_{res}}{SS_{tot}}$$

> From [wiki](https://en.wikipedia.org/wiki/Coefficient_of_determination),

  > The total sum of squares, $SS_{tot}=\sum_i(y_i-\bar{y})^2$

  > The regression sum of squares (explained sum of squares), $SS_{reg}=\sum_i(f_i-\bar{y})^2$

  > The sum of squares of residuals (residual sum of squares), $SS_{res}=\sum_i(y_i-f_i)^2 = \sum_ie^2_i$

**Adj. R-squared**: This is the adjusted R-squared. It is the coefficient of determination adjusted by sample size and the number of parameters used.
$$\bar{R}^2=1-(1-R^2)\frac{n-1}{n-p-1}$$

> $p$ = The total number of explanatory variables not including the constant term

> $n$ = The sample size

**F-statistic**: A measure that tells you if you model is different from a simple average.

**Prob (F-statistic)**: This measures the significance of your F-statistic. Also called p-value of F-statistic. In statistics, p-value equal or lower than 0.05 is considered significant.

**AIC**: This is the Akaike Information Criterion. It evaluatess the model based on the model complexity and number of observations. The lower the better. 

**BIC**: This is the Bayesian Information Criterion. Similar to AIC, except it pushishes models with more parameters.

## Parameters Estimates and the Associated Statistical Tests

**coef**: The estimated coefficient. Note that this is just a point estimate.

**std err**: The standard error of the estimate of the coefficient. Another term for standard deviation

**t**: The t-statistic score. 

**P > |t|**: The p-value. A measure of the probability that the coefficient is different from zero.

**[95.0% Conf. Interval]**: The 95% confidence interval of the coefficient. Shown here as [0.025, 0.975], the lower and upper bound.

## Residual Tests

**Omnibus D'Angostino's test**: This is a combined statistical test for skewness and kurtosis.

**Prob(Omnibus)**: p-value of Omnibus test.

**Skewness**: This is a measure of the symmetry of the residuals around the mean. Zero if symmetrical. A positive value indicates a long tail to the right; a negative value a long tail to the left.

**Kurtosis**: This is a measure of the shape of the distribution of the residuals. A normal distribution has a zero measure. A negative value points to a flatter than normal distribution; a positive one has a higher peak than normal distribution.

**Durbin-Watson**: This is a test for the presence of correlation among the residuals. This is especially important for time series modelling

**Jarque-Bera**: This is a combined statistical test of skewness and kurtosis.

**Prob (JB)**: p-value of Jarque-Bera.

**Cond. No**: This is a test for multicollinearity. > 30 indicates unstable results

***

# statsmodels.formula.api
"""

form_lr = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', 
              data=df)
all_attrs = form_lr.fit()
print (all_attrs.summary())

"""## Exercise

Create a model using the following features

CRIM, ZN, CHAS, NOX
"""

form_lr = smf.ols(formula = 'y ~ CRIM + ZN + CHAS + NOX', data=df)
ex_attr = form_lr.fit()
print (ex_attr.summary())

linear_reg = smf.ols(formula = 'y ~ CRIM + INDUS + RM + LSTAT', data=df)
sel_attr = linear_reg.fit()
print (sel_attr.summary())

"""***

# Correlation Matrix

Useful diagnostic tool to identify collinearity between predictors
"""

pd.options.display.float_format = '{:,.2f}'.format
corr_matrix = df.corr()
corr_matrix

corr_matrix[np.abs(corr_matrix) < 0.6] = 0
corr_matrix

plt.figure(figsize=(12,8))
sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')
plt.show()

"""## Detecting Collinearity with Eigenvectors"""

eigenvalues, eigenvectors = np.linalg.eig(df.corr())

pd.options.display.float_format = '{:,.4f}'.format
pd.Series(eigenvalues).sort_values()

"""Note that index 8, eigenvalue of 0.0635, is near to zero or very small compared to the others. Small value represents presence of collinearity."""

np.abs(pd.Series(eigenvectors[:,8])).sort_values(ascending=False)

"""Note that index 9, 8, 2 have very high loading when compared against the rest"""

print(df.columns[2], df.columns[8], df.columns[9])

"""These are the factors that are causing multicollinearity problem.

***

# Revisiting Feature Importance and Extractions

Check:

1. Direction of the coefficient
2. Impact of the variable / factor on the model
"""

df.head()

plt.hist(df['TAX']);

plt.hist(df['NOX']);

"""## Standardise Variable to Identify Key Feature(s)

In order to perform point 2 properly, one needs to standardise the variable
"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(X,y)

result = pd.DataFrame(list(zip(model.coef_, df.columns)), columns=['coefficient', 'name']).set_index('name')
np.abs(result).sort_values(by='coefficient', ascending=False)

from sklearn.preprocessing import StandardScaler  
from sklearn.pipeline import make_pipeline  
scaler = StandardScaler()  
standard_coefficient_linear_reg = make_pipeline(scaler, model)

standard_coefficient_linear_reg.fit(X,y)
result = pd.DataFrame(list(zip(standard_coefficient_linear_reg.steps[1][1].coef_, df.columns)), 
                      columns=['coefficient', 'name']).set_index('name')
np.abs(result).sort_values(by='coefficient', ascending=False)

"""# Use $R^2$ to Identify Key Features

* Compare $R^2$ of model against $R^2$ of model without a feature. 

* A significant change in $R^2$ signify the importance of the feature.
"""

